{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGUdemy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FFD9A0B380>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FFD97D4050>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of the complex question into smaller sub-questions:\n",
      "\n",
      "1. **What types of memory mechanisms does LangChain offer for its models?**  (This focuses on LangChain's specific memory capabilities)\n",
      "2. **How do LangChain agents utilize memory in their decision-making processes?** (This delves into the practical application of memory within LangChain agents)\n",
      "3. **What memory and retrieval strategies does CrewAI employ for its AI agents?** (This shifts the focus to CrewAI's approach to memory)\n",
      "4. **How do the memory and agent functionalities of LangChain and CrewAI differ in terms of implementation and capabilities?** (This encourages a comparative analysis) \n",
      "\n",
      "\n",
      "These sub-questions target different aspects of the original query, allowing for more focused and precise document retrieval. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions to break down the complex question:\n",
      "A: Please provide the complex question! I need to know what question you're trying to break down before I can provide sub-questions. \n",
      "\n",
      "For example, you could ask:\n",
      "\n",
      "* \"How does CrewAI work?\"\n",
      "* \"What are the advantages of using LangChain agents?\"\n",
      "* \"Can you give me an example of a multi-step workflow that could be improved by CrewAI?\"\n",
      "\n",
      "\n",
      "Once you give me the complex question, I can help you break it down into more manageable sub-questions.  \n",
      "\n",
      "\n",
      "\n",
      "Q: **What types of memory mechanisms does LangChain employ?** (This focuses on LangChain's specific memory capabilities)\n",
      "A: LangChain uses memory modules such as **ConversationBufferMemory** and **ConversationSummaryMemory**. \n",
      "\n",
      "\n",
      "These modules allow the LLM to remember past conversation turns and summarize long interactions. \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents utilize memory?** (This explores the application of memory within LangChain's agent framework)\n",
      "A: LangChain agents use **context-aware memory** across the steps they take to achieve a goal. \n",
      "\n",
      "This means they can remember and utilize information from previous steps in their planning and execution process, allowing for more sophisticated and coherent task completion. \n",
      "\n",
      "\n",
      "Q: **What memory and agent functionalities are offered by CrewAI?** (This shifts the focus to CrewAI's features)\n",
      "A: While the provided text describes CrewAI agents' purpose, goals, tools, and collaborative framework, it **doesn't explicitly mention specific memory functionalities or other agent functionalities beyond their roles (researcher, planner, executor) and semi-independent operation**. \n",
      "\n",
      "\n",
      "To answer your question about CrewAI's memory and agent functionalities, you would need additional information from the documentation or other sources. \n",
      "\n",
      "\n",
      "Q: **What are the key differences in how LangChain and CrewAI handle memory and agent interactions?** (This prompts a comparative analysis)\n",
      "A: Based on the provided context, here's a comparative analysis of how LangChain and CrewAI handle memory and agent interactions:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Memory:** LangChain agents utilize **context-aware memory** that persists across steps in a sequence of tool invocations. This means the agent can remember previous interactions and use that information to make decisions in subsequent steps. \n",
      "* **Agent Interactions:** LangChain agents operate on a **planner-executor model**. The planner determines the sequence of tools to use based on the goal and the available context. The executor then carries out these instructions, interacting with the tools and updating the memory accordingly.  LangChain emphasizes **dynamic decision-making** and **branching logic** within this framework.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Memory:** While the context mentions CrewAI managing **role-based collaboration**, it doesn't explicitly detail how memory is handled within its system. We can infer that CrewAI likely maintains some form of memory specific to each role, allowing them to track their individual tasks and interactions.\n",
      "* **Agent Interactions:** CrewAI focuses on **role-based collaboration**, suggesting a structure where agents (or entities) take on specific roles with defined responsibilities. Interactions likely revolve around these roles, with CrewAI potentially orchestrating communication and task allocation between them.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "* **Memory Scope:** LangChain's memory is centralized and shared by the agent, while CrewAI's memory might be distributed across individual roles.\n",
      "* **Decision-making:** LangChain emphasizes dynamic planning and decision-making within a single agent, whereas CrewAI's focus on roles suggests a more structured, collaborative decision-making process.\n",
      "\n",
      "\n",
      "**Further Clarification Needed:**\n",
      "\n",
      "To provide a more comprehensive comparison, we'd need more information about CrewAI's internal workings, particularly regarding its memory management and how it handles agent interactions beyond the basic concept of roles.\n",
      "\n",
      "\n",
      "Q: These sub-questions target specific aspects of memory and agents in both LangChain and CrewAI, allowing for more focused and accurate document retrieval\n",
      "A: The provided context highlights the strengths of both LangChain and CrewAI in handling memory and agent functionalities:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Hybrid Retrieval:** Leverages both keyword-based and embedding-based retrieval methods, improving recall by capturing both exact matches and semantically similar content. This suggests LangChain's memory system can effectively store and retrieve information based on various criteria.\n",
      "* **Agent Planner-Executor Model:**  LangChain agents use a structured approach to task completion. They plan a sequence of tool invocations, demonstrating an ability to learn from past interactions and adapt their approach based on context. The mention of \"context-aware memory use\" further emphasizes LangChain's capacity to retain and utilize relevant information across multiple steps.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Role-Based Collaboration:**  CrewAI focuses on managing collaboration between different AI \"roles.\" This implies a sophisticated memory system capable of tracking individual roles, their responsibilities, and past interactions.\n",
      "\n",
      "**Combined Strengths:**\n",
      "\n",
      "* **Hybrid Systems:** The context emphasizes the compatibility of LangChain and CrewAI, allowing them to work together. LangChain's retrieval and tool handling capabilities can be combined with CrewAI's role-based collaboration, creating powerful hybrid systems.\n",
      "\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "Both LangChain and CrewAI possess robust memory and agent functionalities.  LangChain excels in information retrieval and planning, while CrewAI specializes in managing collaborative interactions. Their combined strengths create a synergistic effect, enabling the development of sophisticated AI systems. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
