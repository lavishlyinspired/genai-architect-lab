{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTroqVVzluHj"
      },
      "source": [
        "### ASTRADB VectorStore\n",
        "Go from app idea to production with the AI Platform with Astra DB, the ultra-low latency database made for AI and Langflow, the low-code RAG IDE\n",
        "https://www.datastax.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CG4O8j5_iWmQ"
      },
      "outputs": [],
      "source": [
        "### ASTRADB VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sf3FMT8yiEfx",
        "outputId": "d4e679b7-079a-4d7b-c395-24ec4f882ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain<0.4,>=0.3.23 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.52 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (0.3.75)\n",
            "Collecting langchain-astradb<0.7,>=0.6\n",
            "  Using cached langchain_astradb-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (0.4.26)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain<0.4,>=0.3.23) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3.52) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3.52) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3.52) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3.52) (25.0)\n",
            "Collecting astrapy<3.0.0,>=2.0.1 (from langchain-astradb<0.7,>=0.6)\n",
            "  Using cached astrapy-2.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langchain-astradb<0.7,>=0.6) (2.3.2)\n",
            "Collecting deprecation<2.2.0,>=2.1.0 (from astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6) (0.28.1)\n",
            "Collecting pymongo>=3 (from astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached pymongo-4.14.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
            "Collecting toml<0.11.0,>=0.10.2 (from astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting uuid6>=2024.1.12 (from astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached uuid6-2025.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.52) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3.23) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3.23) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3.23) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3.23) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3.23) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3.23) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4,>=0.3.23) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4,>=0.3.23) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4,>=0.3.23) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4,>=0.3.23) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<0.4,>=0.3.23) (3.2.4)\n",
            "Requirement already satisfied: anyio in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6) (0.16.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6)\n",
            "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in h:\\akash\\git\\rag-techniques\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb<0.7,>=0.6) (1.3.1)\n",
            "Using cached langchain_astradb-0.6.1-py3-none-any.whl (58 kB)\n",
            "Using cached astrapy-2.0.1-py3-none-any.whl (300 kB)\n",
            "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached pymongo-4.14.1-cp312-cp312-win_amd64.whl (904 kB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached uuid6-2025.0.1-py3-none-any.whl (7.0 kB)\n",
            "Using cached dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
            "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
            "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: uuid6, toml, hyperframe, hpack, dnspython, deprecation, pymongo, h2, astrapy, langchain-astradb\n",
            "Successfully installed astrapy-2.0.1 deprecation-2.1.0 dnspython-2.8.0 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 langchain-astradb-0.6.1 pymongo-4.14.1 toml-0.10.2 uuid6-2025.0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install \\\n",
        "    \"langchain>=0.3.23,<0.4\" \\\n",
        "    \"langchain-core>=0.3.52,<0.4\" \\\n",
        "    \"langchain-astradb>=0.6,<0.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoR9wNN4iaf3",
        "outputId": "e7704b4f-7b5d-40da-8a5a-1f57134d2673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.98.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.4.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.28\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        #api_key=os.environ[\"GEMINI_API_KEY\"]  # uses GOOGLE_API_KEY env var by default\n",
        "    )\n",
        "\n",
        "# # Initialize Gemini LLM and Google embeddings\n",
        "# llm = ChatGoogleGenerativeAI(\n",
        "#     model=\"gemini-1.5-flash\", \n",
        "#     temperature=0,\n",
        "#     convert_system_message_to_human=True  # Important for Gemini compatibility\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm doing well, thank you for asking.\\n\\nHow are you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--554971b8-a035-4898-a762-3929f9b1cb1b-0', usage_metadata={'input_tokens': 7, 'output_tokens': 407, 'total_tokens': 414, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 388}})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"Hello, how are you?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DdbGUdd-iacy"
      },
      "outputs": [],
      "source": [
        "### Config\n",
        "# ASTRA_DB_API_ENDPOINT=\"https://1beaf365-fca9-432e-8d11-e6bd566158-east-2.apps.astra.datastax.com\"\n",
        "# ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:UQibHoJlupgvUnACFizHwATs1745618f6b2a9046b25a72378dab3f83869a940f4015681811ddb67cc438f1\"\n",
        "os.environ[\"ASTRA_DB_API_ENDPOINT\"]=os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
        "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWA0bZTOiaZh"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=1024,api_key=\"XXXX")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bmQZYiniaWV",
        "outputId": "52a27988-efcd-4ff5-c4db-eff3a7aeec1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FC87F29160>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x000001FC87F2A5D0>, model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUmrERG9iaQd",
        "outputId": "78a22d62-27c9-4daa-83b8-659fd24fa6f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_astradb.vectorstores.AstraDBVectorStore at 0x1fc88401970>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_astradb import AstraDBVectorStore\n",
        "vector_store=AstraDBVectorStore(\n",
        "    embedding=embeddings,\n",
        "    api_endpoint=os.environ[\"ASTRA_DB_API_ENDPOINT\"],\n",
        "    collection_name=\"astra_vector_langchain\",\n",
        "    token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
        "    namespace=None,\n",
        "\n",
        ")\n",
        "vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwoez06VkPVP",
        "outputId": "c8132aa3-4564-4d73-c64d-0baf1406c723"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'tweet'}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.'),\n",
              " Document(metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
              " Document(metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
              " Document(metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.'),\n",
              " Document(metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\"),\n",
              " Document(metadata={'source': 'website'}, page_content='Is the new iPhone worth the price? Read this review to find out.'),\n",
              " Document(metadata={'source': 'website'}, page_content='The top 10 soccer players in the world right now.'),\n",
              " Document(metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
              " Document(metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'),\n",
              " Document(metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :(')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUz80mVRiaKt",
        "outputId": "98ce58e9-dc64-4529-aceb-d40a9fecdff0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['89a1569d88624959972d9d1182878106',\n",
              " '3ce56e023c2242e68f6bd10936da36ce',\n",
              " '59aee6d9fd2446249cd5a661050e2e5c',\n",
              " '037faae865b74939a26add3b95c02786',\n",
              " 'e643bca155c94a8b84a72e3a5446cb26',\n",
              " 'd1bcb44228164cd5ace829e9e1b1bd42',\n",
              " 'f2a0386948ea450d9ace27c9d7bf4b38',\n",
              " '35a3889f2aa343a98b75428312a583ef',\n",
              " '8ac1dae1f6384066af9bd4b2b67e1b62',\n",
              " '45d4dfcfd14e410da92b380815a397ef']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.add_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pny4p7VdiaBw",
        "outputId": "55c9384c-cb31-40c7-e4ec-07342ed37426"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='baaba219104847a1bbf0352788722fc9', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
              " Document(id='3ce56e023c2242e68f6bd10936da36ce', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
              " Document(id='83c7d2c63f654c7fa9afe7925ae1f155', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :('),\n",
              " Document(id='45d4dfcfd14e410da92b380815a397ef', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :(')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Search from Vector Store DB\n",
        "\n",
        "vector_store.similarity_search(\"What is the weather\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63rta9Sfk1vZ",
        "outputId": "7caeee76-a3cd-426d-e522-823f85fbe6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* \"Building an exciting new project with LangChain - come check it out!\", metadata={'source': 'tweet'}\n",
            "* \"LangGraph is the best framework for building stateful, agentic applications!\", metadata={'source': 'tweet'}\n",
            "* \"I have a bad feeling I am going to get deleted :(\", metadata={'source': 'tweet'}\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=3,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f'* \"{res.page_content}\", metadata={res.metadata}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faIybJwglD1O",
        "outputId": "e25d5325-4961-4247-c448-83c788bb67fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* [SIM=0.92] \"Building an exciting new project with LangChain - come check it out!\", metadata={'source': 'tweet'}\n",
            "* [SIM=0.92] \"LangGraph is the best framework for building stateful, agentic applications!\", metadata={'source': 'tweet'}\n",
            "* [SIM=0.86] \"I have a bad feeling I am going to get deleted :(\", metadata={'source': 'tweet'}\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=3,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f'* [SIM={score:.2f}] \"{res.page_content}\", metadata={res.metadata}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6I81FX7lIKu",
        "outputId": "c1c3cfa3-aa2f-415b-bd58-3d58dfc57099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='56a6eaaef1724afaafdcbf14d231010a', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Retriever\n",
        "retriever=vector_store.as_retriever(\n",
        "  search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro267A9ulY82",
        "outputId": "c259395a-05d8-4534-e299-c7982f584fbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='56a6eaaef1724afaafdcbf14d231010a', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Retriever\n",
        "retriever=vector_store.as_retriever(\n",
        "  search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History-aware retriever contextualize_q_prompt! input_variables=['chat_history', 'input'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FCFCED7F60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question \\nwhich might reference context in the chat history, formulate a standalone question \\nwhich can be understood without the chat history. Do NOT answer the question, \\njust reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n",
            "History-aware retriever history_aware_retriever! bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
            "| VectorStoreRetriever(tags=['AstraDBVectorStore', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_astradb.vectorstores.AstraDBVectorStore object at 0x000001FC88401970>, search_type='mmr', search_kwargs={'k': 1}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FCFCED7F60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question \\nwhich might reference context in the chat history, formulate a standalone question \\nwhich can be understood without the chat history. Do NOT answer the question, \\njust reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
            "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FC86ADA870>, default_metadata=(), model_kwargs={})\n",
            "| StrOutputParser()\n",
            "| VectorStoreRetriever(tags=['AstraDBVectorStore', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_astradb.vectorstores.AstraDBVectorStore object at 0x000001FC88401970>, search_type='mmr', search_kwargs={'k': 1})) kwargs={} config={'run_name': 'chat_retriever_chain'} config_factories=[]\n",
            "History-aware retriever created!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create the contextualization prompt\n",
        "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \n",
        "which might reference context in the chat history, formulate a standalone question \n",
        "which can be understood without the chat history. Do NOT answer the question, \n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Create history-aware retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "print(\"History-aware retriever contextualize_q_prompt!\", contextualize_q_prompt)\n",
        "print(\"History-aware retriever history_aware_retriever!\", history_aware_retriever)\n",
        "print(\"History-aware retriever created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversational RAG chain created!\n",
            "\n",
            "============================================================\n",
            "CONVERSATIONAL RAG DEMO\n",
            "============================================================\n",
            "\n",
            "1. First Question:\n",
            "Q: What is machine learning?\n",
            "A: LangGraph is a framework designed for building stateful, agentic applications. It is described as the best framework for this specific purpose.\n",
            "Sources used: 1 documents\n",
            "chat_history: [HumanMessage(content='What is Langraph?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangGraph is a framework designed for building stateful, agentic applications. It is described as the best framework for this specific purpose.', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "2. Follow-up Question:\n",
            "Q: What are its main types?\n",
            "A: Based on the provided context, the main types of LangGraph are not mentioned. The context only states that LangGraph is a framework for building stateful, agentic applications.\n",
            "Sources used: 1 documents\n",
            "chat_history: [HumanMessage(content='What is Langraph?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangGraph is a framework designed for building stateful, agentic applications. It is described as the best framework for this specific purpose.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are its main types?', additional_kwargs={}, response_metadata={}), AIMessage(content='Based on the provided context, the main types of LangGraph are not mentioned. The context only states that LangGraph is a framework for building stateful, agentic applications.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create the QA system prompt\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
        "Use the following pieces of retrieved context to answer the question. \n",
        "If you don't know the answer, just say that you don't know. \n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Context: {context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Create the question-answer chain\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# Create the complete conversational RAG chain\n",
        "conversational_rag_chain = create_retrieval_chain(\n",
        "    history_aware_retriever, \n",
        "    question_answer_chain\n",
        ")\n",
        "\n",
        "print(\"Conversational RAG chain created!\")\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history = []\n",
        "\n",
        "def ask_question(question, chat_history):\n",
        "    \"\"\"Ask a question and update chat history\"\"\"\n",
        "    result = conversational_rag_chain.invoke({\n",
        "        \"chat_history\": chat_history,\n",
        "        \"input\": question\n",
        "    })\n",
        "    \n",
        "    # Update chat history\n",
        "    chat_history.extend([\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=result['answer'])\n",
        "    ])\n",
        "    \n",
        "    return result, chat_history\n",
        "\n",
        "# Example conversation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONVERSATIONAL RAG DEMO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# First question\n",
        "print(\"\\n1. First Question:\")\n",
        "result1, chat_history = ask_question(\"What is Langraph?\", chat_history)\n",
        "print(f\"Q: What is Langraph?\")\n",
        "print(f\"A: {result1['answer']}\")\n",
        "print(f\"Sources used: {len(result1['context'])} documents\")\n",
        "print(f\"chat_history: {chat_history}\")\n",
        "\n",
        "\n",
        "# Follow-up question that references the previous context\n",
        "print(\"\\n2. Follow-up Question:\")\n",
        "result2, chat_history = ask_question(\"What are its main types?\", chat_history)\n",
        "print(f\"Q: What are its main types?\")\n",
        "print(f\"A: {result2['answer']}\")\n",
        "print(f\"Sources used: {len(result2['context'])} documents\")\n",
        "print(f\"chat_history: {chat_history}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l08jC-ublbqr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
