{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3679b0",
   "metadata": {},
   "source": [
    "### üß† What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "‚ÄúIs this clear, complete, and accurate?‚Äù\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, we‚Äôll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f3ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load llm models\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc7a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"internal_docs.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2246d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. State Definition\n",
    "# -------------------------\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    reflection: str = \"\"\n",
    "    revised: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a454cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------\n",
    "\n",
    "# a. Retrieve\n",
    "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\": docs})\n",
    "\n",
    "# b. Generate Answer\n",
    "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "Use the following context to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{state.question}\n",
    "\"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de27da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflect\n",
    "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Reflect on the following answer to see if it fully addresses the question. \n",
    "State YES if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Answer: {state.answer}\n",
    "\n",
    "Respond like:\n",
    "Reflection: YES or NO\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a07cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Finalizer\n",
    "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eafcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. LangGraph DAG\n",
    "# -------------------------\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c3ed5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Answer:\n",
      " The transformer variants mentioned in the production deployments are:\n",
      "\n",
      "1. EfficientFormer\n",
      "3. Reformer\n",
      "4. LLaMA2\n",
      "5. TinyBERT\n",
      "\n",
      "üîÅ Reflection Log:\n",
      " Reflection: NO  \n",
      "Explanation: While the answer lists some transformer variants such as EfficientFormer, Reformer, LLaMA2, and TinyBERT, it is incomplete. There are numerous other transformer variants widely used in production deployments that are not mentioned, such as BERT, GPT-3, DistilBERT, RoBERTa, and T5. The answer should include a broader range of examples to fully address the question. Additionally, it should provide some context or explanation for why these specific variants are notable in the context of production usage.\n",
      "üîÑ Total Attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nüß† Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nüîÅ Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
